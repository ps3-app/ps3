{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection,naive_bayes,svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans,DBSCAN,SpectralClustering,MeanShift\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# utility function for text cleaning and metrics \n",
    "from utils import *\n",
    "from math import log\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans,DBSCAN,SpectralClustering,MeanShift\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import time\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make to function, preprocess, and to calculate cosine similarity\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    non_word='[^a-zA-Z]'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text=re.sub(non_word,' ',parsed_text)\n",
    "    return parsed_text\n",
    "#Cosine similarity function\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Non Hate Speech 11126\n",
      "#Hate speech 737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Corpus=pd.read_csv('final_dataset.csv')\n",
    "#histogram of the label\n",
    "Corpus.hist(column='label')\n",
    "Corpus.dropna(inplace=True)\n",
    "#print the len of hate speech and non hate speech\n",
    "print(\"#Non Hate Speech\",len(Corpus[Corpus['label']==0]))\n",
    "print(\"#Hate speech\",len(Corpus[Corpus['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n",
      "1186\n",
      "9491\n"
     ]
    }
   ],
   "source": [
    "#initialize stop words\n",
    "stop = stopwords.words('english')\n",
    "#shuffle the dataset\n",
    "from sklearn.utils import shuffle\n",
    "Corpus = shuffle(Corpus)\n",
    "#remove stopwords before preprocess\n",
    "Corpus['text'] = Corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#preprocess the text\n",
    "processed_text=[]\n",
    "for t in Corpus['text']:\n",
    "    tokens = preprocess(t)\n",
    "    processed_text.append(tokens)\n",
    "Corpus['text']=processed_text\n",
    "\n",
    "#Split the dataset into Train (10%), Test(20%) and Unlabel (70%)\n",
    "train=Corpus[:1186]\n",
    "test=Corpus[1186:2372]\n",
    "gold_standard=Corpus[2372:]\n",
    "unlabel=gold_standard.copy()\n",
    "unlabel.drop(['label'],axis=1,inplace=True)\n",
    "#train.drop(['text'],axis=1,inplace=True)\n",
    "#test.drop(['text'],axis=1,inplace=True)\n",
    "#save the gold standard label\n",
    "gold_standard.to_csv(\"result/gold_standard.csv\",index=False)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(unlabel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [textTokenizer(text) for text in list(train['text'])]\n",
    "train_labels = list(train['label'])\n",
    "test_texts = [textTokenizer(text) for text in list(test['text'])]\n",
    "test_labels = list(test['label'])\n",
    "unlabel_texts = [textTokenizer(text) for text in list(unlabel['text'])]\n",
    "\n",
    "#encode the label\n",
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(train['label'])\n",
    "y_test=Encoder.fit_transform(test['label'])\n",
    "\n",
    "#TFIDF vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_texts)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_texts)\n",
    "Unlabel_X_Tfidf=Tfidf_vect.transform(unlabel_texts)\n",
    "\n",
    "#get 10% of unlabeled data\n",
    "unlabel_length=round(len(unlabel)*1)\n",
    "#Training and predict the label for unlabel dataset, save the low confidence label generated from the entropy\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "SVM.fit(Train_X_Tfidf,y_train)\n",
    "#predict the probability of class\n",
    "probs=SVM.predict_proba(Unlabel_X_Tfidf)\n",
    "#get the data and then calculate the entropy\n",
    "unlabel_pred_data = list()\n",
    "for j, text, p in zip(unlabel['index'], list(unlabel['text']), probs):\n",
    "    unlabel_pred_data.append([j, text, p[0], p[1]])\n",
    "unlabel_prob_df = pd.DataFrame(unlabel_pred_data, columns=[\"index\", \"text\", \"class_0\", \"class_1\"])\n",
    "entropy=list()\n",
    "for p in probs:\n",
    "    ent=0\n",
    "    ent1=-p[0] * log(p[0],2)\n",
    "    ent2=-p[1] * log(p[1],2)\n",
    "    ent=ent1+ent2\n",
    "    entropy.append(ent)\n",
    "unlabel_prob_df['entropy']=entropy\n",
    "#we already have the low entropy\n",
    "#get 10% data to be clustered\n",
    "unlabel_prob_df=unlabel_prob_df.sort_values(by='entropy',ascending=False)\n",
    "#rank the lower uncertainty\n",
    "#lower_uncertainty=unlabel_prob_df[unlabel_prob_df['entropy']>=0.5]\n",
    "lower_uncertainty=unlabel_prob_df\n",
    "low_conf=lower_uncertainty[0:unlabel_length]\n",
    "#low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "#low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "\n",
    "#save the low conf for checking later\n",
    "#embedd the low conf text \n",
    "\n",
    "#clustering based on uncertainty\n",
    "#print(\"clustering \",iteration)\n",
    "df=low_conf['entropy']\n",
    "uncertain=df.to_numpy()\n",
    "\n",
    "#initialize the kmeans\n",
    "kmeans = KMeans(n_clusters=60, random_state=0,max_iter=600,n_init=10)\n",
    "kmeans.fit(uncertain.reshape(-1,1))\n",
    "labels = (kmeans.labels_)\n",
    "#create the cluster for each low conf data\n",
    "\n",
    "#calculate the center\n",
    "similarity_to_center=list()\n",
    "for i, instance in enumerate(uncertain.reshape(-1,1)):\n",
    "    cluster_label = kmeans.labels_[i]\n",
    "    centroid = kmeans.cluster_centers_[cluster_label] # cluster center of the cluster of that instance\n",
    "\n",
    "    similarity=distance.euclidean(instance,centroid)\n",
    "    similarity_to_center.append(similarity)\n",
    "cluster_labels=pd.DataFrame({\"index\":low_conf['index'],\"text\":low_conf['text'],'cluster':kmeans.labels_,\n",
    "                            'uncertainty':low_conf['entropy'],'similarity':similarity_to_center})\n",
    "#creating cluster_dict\n",
    "cluster_dict=dict()\n",
    "#creating the dataframe for each cluster\n",
    "for item in range(0,60):\n",
    "    cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "#Create the dataframe for each cluster\n",
    "for i in range(0,60):\n",
    "        globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001B049982B00>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_1.hist(column='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "X=cluster_1['similarity']\n",
    "n, bins, patches = plt.hist(x=cluster_1['similarity'], bins=10, color='red',\n",
    "                            alpha=0.7, rwidth=0.45)\n",
    "#v3 = np.concatenate((v1,v2))\n",
    "#sns.kdeplot(cluster_1['similarity']);\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Similarity to Center')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Points Similarity')\n",
    "plt.savefig(\"histogram.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0\n",
      "clustering  0\n",
      "training data 180\n",
      "adding training data 1366\n",
      "9313\n",
      "Calculating performance 0\n",
      "0.6593675947451467\n",
      "start  1\n",
      "clustering  1\n",
      "training data 180\n",
      "adding training data 1545\n",
      "9133\n",
      "Calculating performance 1\n",
      "0.6809755344881552\n",
      "start  2\n",
      "clustering  2\n",
      "training data 180\n",
      "adding training data 1725\n",
      "8953\n",
      "Calculating performance 2\n",
      "0.7226601713214073\n",
      "start  3\n",
      "clustering  3\n",
      "training data 180\n",
      "adding training data 1905\n",
      "8774\n",
      "Calculating performance 3\n",
      "0.6894149467120654\n",
      "start  4\n",
      "clustering  4\n",
      "training data 180\n",
      "adding training data 2085\n",
      "8594\n",
      "Calculating performance 4\n",
      "0.700446286306095\n",
      "start  5\n",
      "clustering  5\n",
      "training data 180\n",
      "adding training data 2265\n",
      "8415\n",
      "Calculating performance 5\n",
      "0.7216749490891707\n",
      "start  6\n",
      "clustering  6\n",
      "training data 180\n",
      "adding training data 2445\n",
      "8235\n",
      "Calculating performance 6\n",
      "0.7106590408356377\n",
      "start  7\n",
      "clustering  7\n",
      "training data 180\n",
      "adding training data 2625\n",
      "8055\n",
      "Calculating performance 7\n",
      "0.7429354783559606\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "gmean=list()\n",
    "scoring=list()\n",
    "auc_score=list()\n",
    "for iteration in range(0,8):\n",
    "    print(\"start \",iteration)\n",
    "    train_texts = [textTokenizer(text) for text in list(train['text'])]\n",
    "    train_labels = list(train['label'])\n",
    "    test_texts = [textTokenizer(text) for text in list(test['text'])]\n",
    "    test_labels = list(test['label'])\n",
    "    unlabel_texts = [textTokenizer(text) for text in list(unlabel['text'])]\n",
    "\n",
    "    #encode the label\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test=Encoder.fit_transform(test['label'])\n",
    "\n",
    "    #TFIDF vectorizer\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(Corpus['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(train_texts)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(test_texts)\n",
    "    Unlabel_X_Tfidf=Tfidf_vect.transform(unlabel_texts)\n",
    "\n",
    "    #get 10% of unlabeled data\n",
    "    unlabel_length=round(len(unlabel)*1)\n",
    "    #Training and predict the label for unlabel dataset, save the low confidence label generated from the entropy\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    #predict the probability of class\n",
    "    probs=SVM.predict_proba(Unlabel_X_Tfidf)\n",
    "    #get the data and then calculate the entropy\n",
    "    unlabel_pred_data = list()\n",
    "    for j, text, p in zip(unlabel['index'], list(unlabel['text']), probs):\n",
    "        unlabel_pred_data.append([j, text, p[0], p[1]])\n",
    "    unlabel_prob_df = pd.DataFrame(unlabel_pred_data, columns=[\"index\", \"text\", \"class_0\", \"class_1\"])\n",
    "    entropy=list()\n",
    "    for p in probs:\n",
    "        ent=0\n",
    "        ent1=-p[0] * log(p[0],2)\n",
    "        ent2=-p[1] * log(p[1],2)\n",
    "        ent=ent1+ent2\n",
    "        entropy.append(ent)\n",
    "    unlabel_prob_df['entropy']=entropy\n",
    "    #we already have the low entropy\n",
    "    #get 10% data to be clustered\n",
    "    unlabel_prob_df=unlabel_prob_df.sort_values(by='entropy',ascending=False)\n",
    "    #rank the lower uncertainty\n",
    "    #lower_uncertainty=unlabel_prob_df[unlabel_prob_df['entropy']>=0.5]\n",
    "    lower_uncertainty=unlabel_prob_df\n",
    "    low_conf=lower_uncertainty[0:unlabel_length]\n",
    "    #low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "    #low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "\n",
    "    #save the low conf for checking later\n",
    "    #embedd the low conf text \n",
    "    \n",
    "    #clustering based on uncertainty\n",
    "    print(\"clustering \",iteration)\n",
    "    df=low_conf['entropy']\n",
    "    uncertain=df.to_numpy()\n",
    "\n",
    "    #initialize the kmeans\n",
    "    kmeans = KMeans(n_clusters=60, random_state=0,max_iter=600,n_init=10)\n",
    "    kmeans.fit(uncertain.reshape(-1,1))\n",
    "    labels = (kmeans.labels_)\n",
    "    #create the cluster for each low conf data\n",
    "\n",
    "    #calculate the center\n",
    "    similarity_to_center=list()\n",
    "    for i, instance in enumerate(uncertain.reshape(-1,1)):\n",
    "        cluster_label = kmeans.labels_[i]\n",
    "        centroid = kmeans.cluster_centers_[cluster_label] # cluster center of the cluster of that instance\n",
    "\n",
    "        similarity=distance.euclidean(instance,centroid)\n",
    "        similarity_to_center.append(similarity)\n",
    "    cluster_labels=pd.DataFrame({\"index\":low_conf['index'],\"text\":low_conf['text'],'cluster':kmeans.labels_,\n",
    "                                'uncertainty':low_conf['entropy'],'similarity':similarity_to_center})\n",
    "    #creating cluster_dict\n",
    "    cluster_dict=dict()\n",
    "    #creating the dataframe for each cluster\n",
    "    for item in range(0,60):\n",
    "        cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "    #Create the dataframe for each cluster\n",
    "    for i in range(0,60):\n",
    "            globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])\n",
    "    for i in range(0,60):\n",
    "        globals()['cluster_{}'.format(i)]=globals()['cluster_{}'.format(i)].sort_values(by=\"similarity\",ascending=False)\n",
    "    center_data=pd.DataFrame()\n",
    "    for i in range(0,60):\n",
    "        data1=globals()['cluster_{}'.format(i)]\n",
    "        mn=data1['similarity'].median()\n",
    "        first=round(len(data1)*0.05)\n",
    "        first_data=data1.iloc[first,4]\n",
    "        second=round(len(data1)*0.95)-1\n",
    "        second_data=data1.iloc[second,4]\n",
    "        cntr1=data1[data1['similarity']<=mn]\n",
    "        ot1=data1[data1['similarity']<=first_data]\n",
    "        ot2=data1[data1['similarity']<=second_data]\n",
    "        cntr1=cntr1.head(1)\n",
    "        ot1=ot1.head(1)\n",
    "        ot2=ot2.head(1)\n",
    "        frame=[cntr1,ot1,ot2]\n",
    "        #data1=globals()['cluster_{}'.format(i)].head(1)\n",
    "        #data2=globals()['cluster_{}'.format(i)].tail(2)\n",
    "        #frame=[data1,data2]\n",
    "        center_data=center_data.append(frame)\n",
    "    center_data.to_csv(\"center_data.csv\",index=False)\n",
    "\n",
    "    #labeling\n",
    "    true_label=pd.read_csv(\"result/gold_standard.csv\")\n",
    "    #get the label\n",
    "    new_train=true_label.merge(center_data,on='index',how='left')\n",
    "    #Drop NaN\n",
    "    new_train=new_train.dropna()\n",
    "    #to check the percentage of minority class in each cluster\n",
    "    #sort by cluster\n",
    "    new_train.sort_values(by=['cluster'],inplace=True)\n",
    "    new_train.to_csv(\"check_minority_class.csv\",index=False)\n",
    "    #Drop duplicate columns\n",
    "    new_train.drop(['cluster','text_y','similarity'],axis=1,inplace=True)\n",
    "    new_train.rename(columns={\"text_x\": \"text\"},inplace=True)\n",
    "    #print(\"Minority Class Captured\",len(new_train[new_train['label']==1]))\n",
    "    #print(len(new_train[new_train['label']==0]))\n",
    "    #print(len(new_train[new_train['label']==1]))\n",
    "    #Get 50 of majority and minority class\n",
    "    \n",
    "    \n",
    "\n",
    "    #new_train.drop(['cluster','uncertainty','similarity'],axis=1,inplace=True)\n",
    "    training_data=pd.DataFrame({'index':new_train['index'],\n",
    "                               'label':new_train['label'],'text':new_train['text']})\n",
    "    print(\"training data {}\".format(len(training_data)))\n",
    "    #calculating performance\n",
    "\n",
    "    #train=train.append(training_data)\n",
    "    train=train.append(training_data)\n",
    "    print(\"adding training data {}\".format(len(train)))\n",
    "\n",
    "    train.to_csv(\"training/training.csv\",index=False)\n",
    "    test.to_csv(\"training/test.csv\",index=False)\n",
    "\n",
    "    #Drop sample from unlabel pool\n",
    "    unlabel.to_csv(\"unlabel.csv\",index=False)\n",
    "    unlabel=unlabel[~unlabel['index'].isin(new_train['index'])]\n",
    "    print(len(unlabel))\n",
    "    print(\"Calculating performance\",iteration)\n",
    "\n",
    "    train=pd.read_csv(\"training/training.csv\")\n",
    "    test=pd.read_csv(\"training/test.csv\")\n",
    "    \n",
    "    minority_len=len(train[train['label']==1])+len(new_train[new_train['label']==1])\n",
    "    majority_len=len(train[train['label']==0])+len(new_train[new_train['label']==0])\n",
    "    \n",
    "    minority_weight=majority_len/minority_len\n",
    "    majority_weight=minority_len/minority_len\n",
    "    \n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test = Encoder.fit_transform(test['label'])\n",
    "    X_train=train['text']\n",
    "    X_test=test['text']\n",
    "\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(train['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "    SVM = svm.SVC(C=1, kernel='linear', gamma='auto',probability=True,class_weight={0:majority_weight,1:minority_weight})\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    score=geometric_mean_score(y_test, predictions_SVM)\n",
    "    auc=roc_auc_score(y_test,predictions_SVM)\n",
    "\n",
    "    print(score)\n",
    "    scoring.append(score)\n",
    "    auc_score.append(auc)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6593675947451467,\n",
       " 0.6809755344881552,\n",
       " 0.7226601713214073,\n",
       " 0.6894149467120654,\n",
       " 0.700446286306095,\n",
       " 0.7216749490891707,\n",
       " 0.7106590408356377,\n",
       " 0.7429354783559606]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
