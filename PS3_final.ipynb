{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dataPreparation as dp\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from math import log\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.spatial import distance\n",
    "from sklearn.mixture import GaussianMixture\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk import pos_tag\n",
    "import nltk\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "#for this experiment we use Founta Dataset\n",
    "Corpus=pd.read_csv('final_dataset.csv')\n",
    "#shuffle the Corpus for better generalization\n",
    "Corpus = shuffle(Corpus)\n",
    "#remove stopwords before preprocess\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus['text'] = Corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#preprocess the text\n",
    "processed_text=[]\n",
    "for t in Corpus['text']:\n",
    "    tokens = dp.DataPreparation.preprocess(t)\n",
    "    processed_text.append(tokens)\n",
    "Corpus['text']=processed_text\n",
    "\n",
    "#Split the dataset into Train (10%), Test(20%) and Unlabel (70%)\n",
    "train=Corpus[:1186]\n",
    "test=Corpus[1186:2372]\n",
    "gold_standard=Corpus[2372:]\n",
    "unlabel=gold_standard.copy()\n",
    "unlabel.drop(['label'],axis=1,inplace=True)\n",
    "#save the gold standard\n",
    "gold_standard.to_csv(\"result/gold_standard.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Annotation Iteration :  0\n",
      "Processing Annotation Iteration :  1\n",
      "Processing Annotation Iteration :  2\n",
      "Processing Annotation Iteration :  3\n",
      "Processing Annotation Iteration :  4\n",
      "Processing Annotation Iteration :  5\n",
      "Processing Annotation Iteration :  6\n",
      "Processing Annotation Iteration :  7\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#GMM without balancing\n",
    "gmean=list()\n",
    "scoring=list()\n",
    "auc_score=list()\n",
    "#we take 8 iteration as it simulize full 8 hours of working days of annotation\n",
    "for iteration in range(0,8):\n",
    "    print(\"Processing Annotation Iteration : \",iteration)\n",
    "    train_texts = [textTokenizer(text) for text in list(train['text'])]\n",
    "    train_labels = list(train['label'])\n",
    "    test_texts = [textTokenizer(text) for text in list(test['text'])]\n",
    "    test_labels = list(test['label'])\n",
    "    unlabel_texts = [textTokenizer(text) for text in list(unlabel['text'])]\n",
    "\n",
    "    #encode the label\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test=Encoder.fit_transform(test['label'])\n",
    "\n",
    "    #TFIDF vectorizer\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(Corpus['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(train_texts)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(test_texts)\n",
    "    Unlabel_X_Tfidf=Tfidf_vect.transform(unlabel_texts)\n",
    "\n",
    "    \n",
    "    unlabel_length=round(len(unlabel)*1)\n",
    "    #Training and predict the label for unlabel dataset, save the low confidence label generated from the entropy\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    #predict the probability of class\n",
    "    probs=SVM.predict_proba(Unlabel_X_Tfidf)\n",
    "    #get the data and then calculate the uncertainty of data points\n",
    "    unlabel_pred_data = list()\n",
    "    for j, text, p in zip(unlabel['index'], list(unlabel['text']), probs):\n",
    "        unlabel_pred_data.append([j, text, p[0], p[1]])\n",
    "    unlabel_prob_df = pd.DataFrame(unlabel_pred_data, columns=[\"index\", \"text\", \"class_0\", \"class_1\"])\n",
    "    entropy=list()\n",
    "    for p in probs:\n",
    "        ent=0\n",
    "        ent1=-p[0] * log(p[0],2)\n",
    "        ent2=-p[1] * log(p[1],2)\n",
    "        ent=ent1+ent2\n",
    "        entropy.append(ent)\n",
    "    unlabel_prob_df['entropy']=entropy\n",
    "   \n",
    "    #get 10% data to be clustered\n",
    "    unlabel_prob_df=unlabel_prob_df.sort_values(by='entropy',ascending=False)\n",
    "    #rank the lower uncertainty\n",
    "   \n",
    "    lower_uncertainty=unlabel_prob_df\n",
    "    low_conf=lower_uncertainty[0:unlabel_length]\n",
    "    \n",
    "\n",
    "    #save the low conf for checking later\n",
    "    #embedd the low conf text \n",
    "    \n",
    "    #clustering based on uncertainty\n",
    "    df=low_conf['entropy']\n",
    "    uncertain=df.to_numpy()\n",
    "\n",
    "    #initialize the GMM\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=60)\n",
    "    gmm.fit(uncertain.reshape(-1,1))\n",
    "    labels=gmm.predict(uncertain.reshape(-1,1))\n",
    "    similarity_to_center=list()\n",
    "    for i, instance in enumerate(uncertain.reshape(-1,1)):\n",
    "        cluster_label = labels[i]\n",
    "        centroid = gmm.means_[cluster_label] # cluster center of the cluster of that instance\n",
    "\n",
    "        similarity=distance.euclidean(instance,centroid)\n",
    "        similarity_to_center.append(similarity)\n",
    "    cluster_labels=pd.DataFrame({\"index\":low_conf['index'],\"text\":low_conf['text'],'cluster':labels,\n",
    "                                'uncertainty':low_conf['entropy'],'similarity':similarity_to_center})\n",
    "    #creating cluster_dict\n",
    "    cluster_dict=dict()\n",
    "    #creating the dataframe for each cluster\n",
    "    for item in range(0,60):\n",
    "        cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "    #Create the dataframe for each cluster\n",
    "    for i in range(0,60):\n",
    "            globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])\n",
    "    for i in range(0,60):\n",
    "        globals()['cluster_{}'.format(i)]=globals()['cluster_{}'.format(i)].sort_values(by=\"similarity\",ascending=True)\n",
    "    center_data=pd.DataFrame()\n",
    "    \n",
    "    #Tale of Tails\n",
    "    for i in range(0,60):\n",
    "        data1=globals()['cluster_{}'.format(i)]\n",
    "        mn=data1['similarity'].median()\n",
    "        first=round(len(data1)*0.05)\n",
    "        first_data=data1.iloc[first,4]\n",
    "        second=round(len(data1)*0.95)-1\n",
    "        second_data=data1.iloc[second,4]\n",
    "        cntr1=data1[data1['similarity']<=mn]\n",
    "        ot1=data1[data1['similarity']<=first_data]\n",
    "        ot2=data1[data1['similarity']<=second_data]\n",
    "        cntr1=cntr1.head(1)\n",
    "        ot1=ot1.head(1)\n",
    "        ot2=ot2.head(1)\n",
    "        frame=[cntr1,ot1,ot2]\n",
    "        center_data=center_data.append(frame)\n",
    "    \n",
    "\n",
    "    #We imitate the labeling by having an oracle from the true label data\n",
    "    #get the labeling\n",
    "    true_label=pd.read_csv(\"result/gold_standard.csv\")\n",
    "    #get the label\n",
    "    new_train=true_label.merge(center_data,on='index',how='left')\n",
    "    #Drop NaN\n",
    "    new_train=new_train.dropna()\n",
    "    \n",
    "    #sort by cluster\n",
    "    new_train.sort_values(by=['cluster'],inplace=True)\n",
    "   \n",
    "    #Drop duplicate columns\n",
    "    new_train.drop(['cluster','text_y','similarity'],axis=1,inplace=True)\n",
    "    new_train.rename(columns={\"text_x\": \"text\"},inplace=True)\n",
    "    \n",
    "    #Get 50 of majority and minority class\n",
    "    training_data=pd.DataFrame({'index':new_train['index'],\n",
    "                               'label':new_train['label'],'text':new_train['text']})\n",
    "    \n",
    "    #calculating performance\n",
    "    train=train.append(training_data)\n",
    "    train.to_csv(\"training/training.csv\",index=False)\n",
    "    test.to_csv(\"training/test.csv\",index=False)\n",
    "\n",
    "    #Drop sample from unlabel pool\n",
    "    unlabel=unlabel[~unlabel['index'].isin(new_train['index'])]\n",
    "        \n",
    "\n",
    "    train=pd.read_csv(\"training/training.csv\")\n",
    "    test=pd.read_csv(\"training/test.csv\")\n",
    "    #calculate the class weight \n",
    "    \n",
    "    minority_len=len(train[train['label']==1])+len(new_train[new_train['label']==1])\n",
    "    majority_len=len(train[train['label']==0])+len(new_train[new_train['label']==0])\n",
    "    \n",
    "    minority_weight=majority_len/minority_len\n",
    "    majority_weight=minority_len/minority_len\n",
    "    \n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test = Encoder.fit_transform(test['label'])\n",
    "    X_train=train['text']\n",
    "    X_test=test['text']\n",
    "\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(train['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "    SVM = svm.SVC(C=1, kernel='linear', gamma='auto',probability=True,class_weight={0:majority_weight,1:minority_weight})\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    # predict the labels on validation dataset and calculate the GMean Score\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    score=geometric_mean_score(y_test, predictions_SVM)\n",
    "    auc=roc_auc_score(y_test,predictions_SVM)\n",
    "    scoring.append(score)\n",
    "    auc_score.append(auc)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6103832317821816,\n",
       " 0.6209022998890171,\n",
       " 0.6335432003761389,\n",
       " 0.662644425178078,\n",
       " 0.6537233233193875,\n",
       " 0.6635556945866374,\n",
       " 0.6217569490201733,\n",
       " 0.6326755306692329]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the Gmean Result From the experiment\n",
    "scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
