{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection,naive_bayes,svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans,DBSCAN,SpectralClustering,MeanShift\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "# Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# utility function for text cleaning and metrics \n",
    "from utils import *\n",
    "from math import log\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans,DBSCAN,SpectralClustering,MeanShift\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import time\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.spatial import distance\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make to function, preprocess, and to calculate cosine similarity\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    non_word='[^a-zA-Z]'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text=re.sub(non_word,' ',parsed_text)\n",
    "    return parsed_text\n",
    "#Cosine similarity function\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Non Hate Speech 11126\n",
      "#Hate speech 737\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "Corpus=pd.read_csv('final_dataset.csv')\n",
    "#histogram of the label\n",
    "Corpus.hist(column='label')\n",
    "Corpus.dropna(inplace=True)\n",
    "#print the len of hate speech and non hate speech\n",
    "print(\"#Non Hate Speech\",len(Corpus[Corpus['label']==0]))\n",
    "print(\"#Hate speech\",len(Corpus[Corpus['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n",
      "1186\n",
      "9491\n"
     ]
    }
   ],
   "source": [
    "#initialize stop words\n",
    "stop = stopwords.words('english')\n",
    "#shuffle the dataset\n",
    "from sklearn.utils import shuffle\n",
    "Corpus = shuffle(Corpus)\n",
    "#remove stopwords before preprocess\n",
    "Corpus['text'] = Corpus['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#preprocess the text\n",
    "processed_text=[]\n",
    "for t in Corpus['text']:\n",
    "    tokens = preprocess(t)\n",
    "    processed_text.append(tokens)\n",
    "Corpus['text']=processed_text\n",
    "\n",
    "#Split the dataset into Train (10%), Test(20%) and Unlabel (70%)\n",
    "train=Corpus[:1186]\n",
    "test=Corpus[1186:2372]\n",
    "gold_standard=Corpus[2372:]\n",
    "unlabel=gold_standard.copy()\n",
    "unlabel.drop(['label'],axis=1,inplace=True)\n",
    "#train.drop(['text'],axis=1,inplace=True)\n",
    "#test.drop(['text'],axis=1,inplace=True)\n",
    "#save the gold standard label\n",
    "gold_standard.to_csv(\"result/gold_standard.csv\",index=False)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(len(unlabel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  0\n",
      "clustering  0\n",
      "training data 175\n",
      "adding training data 1361\n",
      "9316\n",
      "Calculating performance 0\n",
      "0.506409292855323\n",
      "start  1\n",
      "clustering  1\n",
      "training data 174\n",
      "adding training data 1534\n",
      "9142\n",
      "Calculating performance 1\n",
      "0.5338022645660181\n",
      "start  2\n",
      "clustering  2\n",
      "training data 173\n",
      "adding training data 1707\n",
      "8969\n",
      "Calculating performance 2\n",
      "0.5335619754408264\n",
      "start  3\n",
      "clustering  3\n",
      "training data 173\n",
      "adding training data 1880\n",
      "8796\n",
      "Calculating performance 3\n",
      "0.5467383292930292\n",
      "start  4\n",
      "clustering  4\n",
      "training data 174\n",
      "adding training data 2054\n",
      "8622\n",
      "Calculating performance 4\n",
      "0.5338022645660181\n",
      "start  5\n",
      "clustering  5\n",
      "training data 177\n",
      "adding training data 2231\n",
      "8445\n",
      "Calculating performance 5\n",
      "0.5469845523765727\n",
      "start  6\n",
      "clustering  6\n",
      "training data 177\n",
      "adding training data 2408\n",
      "8268\n",
      "Calculating performance 6\n",
      "0.5593523899819391\n",
      "start  7\n",
      "clustering  7\n",
      "training data 177\n",
      "adding training data 2585\n",
      "8091\n",
      "Calculating performance 7\n",
      "0.546245550166955\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#GMM without balancing\n",
    "gmean=list()\n",
    "scoring=list()\n",
    "auc_score=list()\n",
    "for iteration in range(0,8):\n",
    "    print(\"start \",iteration)\n",
    "    train_texts = [textTokenizer(text) for text in list(train['text'])]\n",
    "    train_labels = list(train['label'])\n",
    "    test_texts = [textTokenizer(text) for text in list(test['text'])]\n",
    "    test_labels = list(test['label'])\n",
    "    unlabel_texts = [textTokenizer(text) for text in list(unlabel['text'])]\n",
    "\n",
    "    #encode the label\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test=Encoder.fit_transform(test['label'])\n",
    "\n",
    "    #TFIDF vectorizer\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(Corpus['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(train_texts)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(test_texts)\n",
    "    Unlabel_X_Tfidf=Tfidf_vect.transform(unlabel_texts)\n",
    "\n",
    "    #get 10% of unlabeled data\n",
    "    unlabel_length=round(len(unlabel)*1)\n",
    "    #Training and predict the label for unlabel dataset, save the low confidence label generated from the entropy\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    #predict the probability of class\n",
    "    probs=SVM.predict_proba(Unlabel_X_Tfidf)\n",
    "    #get the data and then calculate the entropy\n",
    "    unlabel_pred_data = list()\n",
    "    for j, text, p in zip(unlabel['index'], list(unlabel['text']), probs):\n",
    "        unlabel_pred_data.append([j, text, p[0], p[1]])\n",
    "    unlabel_prob_df = pd.DataFrame(unlabel_pred_data, columns=[\"index\", \"text\", \"class_0\", \"class_1\"])\n",
    "    entropy=list()\n",
    "    for p in probs:\n",
    "        ent=0\n",
    "        ent1=-p[0] * log(p[0],2)\n",
    "        ent2=-p[1] * log(p[1],2)\n",
    "        ent=ent1+ent2\n",
    "        entropy.append(ent)\n",
    "    unlabel_prob_df['entropy']=entropy\n",
    "    #we already have the low entropy\n",
    "    #get 10% data to be clustered\n",
    "    unlabel_prob_df=unlabel_prob_df.sort_values(by='entropy',ascending=False)\n",
    "    #rank the lower uncertainty\n",
    "    #lower_uncertainty=unlabel_prob_df[unlabel_prob_df['entropy']>=0.5]\n",
    "    lower_uncertainty=unlabel_prob_df\n",
    "    low_conf=lower_uncertainty[0:unlabel_length]\n",
    "    #low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "    #low_conf=unlabel_prob_df[0:unlabel_length]\n",
    "\n",
    "    #save the low conf for checking later\n",
    "    #embedd the low conf text \n",
    "    \n",
    "    #clustering based on uncertainty\n",
    "    print(\"clustering \",iteration)\n",
    "    df=low_conf['entropy']\n",
    "    uncertain=df.to_numpy()\n",
    "\n",
    "    #initialize the GMM\n",
    "    #create the cluster for each low conf data\n",
    "    gmm = GaussianMixture(n_components=180)\n",
    "    gmm.fit(uncertain.reshape(-1,1))\n",
    "    labels=gmm.predict(uncertain.reshape(-1,1))\n",
    "    similarity_to_center=list()\n",
    "    for i, instance in enumerate(uncertain.reshape(-1,1)):\n",
    "        cluster_label = labels[i]\n",
    "        centroid = gmm.means_[cluster_label] # cluster center of the cluster of that instance\n",
    "\n",
    "        similarity=distance.euclidean(instance,centroid)\n",
    "        similarity_to_center.append(similarity)\n",
    "    cluster_labels=pd.DataFrame({\"index\":low_conf['index'],\"text\":low_conf['text'],'cluster':labels,\n",
    "                                'uncertainty':low_conf['entropy'],'similarity':similarity_to_center})\n",
    "    #creating cluster_dict\n",
    "    cluster_dict=dict()\n",
    "    #creating the dataframe for each cluster\n",
    "    for item in range(0,180):\n",
    "        cluster_dict['cluster_{0}'.format(item)]=cluster_labels[cluster_labels['cluster']==item] \n",
    "    #Create the dataframe for each cluster\n",
    "    for i in range(0,180):\n",
    "            globals()['cluster_{}'.format(i)] = pd.DataFrame(cluster_dict['cluster_{}'.format(i)])\n",
    "    for i in range(0,180):\n",
    "        globals()['cluster_{}'.format(i)]=globals()['cluster_{}'.format(i)].sort_values(by=\"similarity\",ascending=True)\n",
    "    center_data=pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,60):\n",
    "        data1=globals()['cluster_{}'.format(i)]\n",
    "        mn=data1['similarity'].median()\n",
    "        first=round(len(data1)*0.05)\n",
    "        first_data=data1.iloc[first,4]\n",
    "        second=round(len(data1)*0.95)-1\n",
    "        second_data=data1.iloc[second,4]\n",
    "        cntr1=data1[data1['similarity']<=mn]\n",
    "        ot1=data1[data1['similarity']<=first_data]\n",
    "        ot2=data1[data1['similarity']<=second_data]\n",
    "        cntr1=cntr1.head(1)\n",
    "        ot1=ot1.head(1)\n",
    "        ot2=ot2.head(1)\n",
    "        frame=[cntr1,ot1,ot2]\n",
    "        #data1=globals()['cluster_{}'.format(i)].head(1)\n",
    "        #data2=globals()['cluster_{}'.format(i)].tail(2)\n",
    "        #frame=[data1,data2]\n",
    "        center_data=center_data.append(frame)\n",
    "    center_data.to_csv(\"center_data.csv\",index=False)\n",
    "\n",
    "\n",
    "    #labeling\n",
    "    true_label=pd.read_csv(\"result/gold_standard.csv\")\n",
    "    #get the label\n",
    "    new_train=true_label.merge(center_data,on='index',how='left')\n",
    "    #Drop NaN\n",
    "    new_train=new_train.dropna()\n",
    "    #to check the percentage of minority class in each cluster\n",
    "    #sort by cluster\n",
    "    new_train.sort_values(by=['cluster'],inplace=True)\n",
    "    new_train.to_csv(\"check_minority_class.csv\",index=False)\n",
    "    #Drop duplicate columns\n",
    "    new_train.drop(['cluster','text_y','similarity'],axis=1,inplace=True)\n",
    "    new_train.rename(columns={\"text_x\": \"text\"},inplace=True)\n",
    "    #print(\"Minority Class Captured\",len(new_train[new_train['label']==1]))\n",
    "    #print(len(new_train[new_train['label']==0]))\n",
    "    #print(len(new_train[new_train['label']==1]))\n",
    "    #Get 50 of majority and minority class\n",
    "    training_data=pd.DataFrame({'index':new_train['index'],\n",
    "                               'label':new_train['label'],'text':new_train['text']})\n",
    "    print(\"training data {}\".format(len(training_data)))\n",
    "    #calculating performance\n",
    "\n",
    "    #train=train.append(training_data)\n",
    "    train=train.append(training_data)\n",
    "    print(\"adding training data {}\".format(len(train)))\n",
    "\n",
    "    train.to_csv(\"training/training.csv\",index=False)\n",
    "    test.to_csv(\"training/test.csv\",index=False)\n",
    "\n",
    "    #Drop sample from unlabel pool\n",
    "    unlabel.to_csv(\"unlabel.csv\",index=False)\n",
    "    unlabel=unlabel[~unlabel['index'].isin(new_train['index'])]\n",
    "    print(len(unlabel))\n",
    "    print(\"Calculating performance\",iteration)\n",
    "\n",
    "    train=pd.read_csv(\"training/training.csv\")\n",
    "    test=pd.read_csv(\"training/test.csv\")\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "\n",
    "    Encoder = LabelEncoder()\n",
    "    y_train = Encoder.fit_transform(train['label'])\n",
    "    y_test = Encoder.fit_transform(test['label'])\n",
    "    X_train=train['text']\n",
    "    X_test=test['text']\n",
    "\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    Tfidf_vect.fit(train['text'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "    SVM = svm.SVC(C=1, kernel='linear', gamma='auto',probability=True)\n",
    "    SVM.fit(Train_X_Tfidf,y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    score=geometric_mean_score(y_test, predictions_SVM)\n",
    "    auc=roc_auc_score(y_test,predictions_SVM)\n",
    "\n",
    "    print(score)\n",
    "    scoring.append(score)\n",
    "    auc_score.append(auc)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.506409292855323,\n",
       " 0.5338022645660181,\n",
       " 0.5335619754408264,\n",
       " 0.5467383292930292,\n",
       " 0.5338022645660181,\n",
       " 0.5469845523765727,\n",
       " 0.5593523899819391,\n",
       " 0.546245550166955]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
